{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from re import search\n",
    "from subprocess import getoutput as gop\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from urllib.request import urlopen, urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    # 'small': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/smni_eeg_data.tar.gz',\n",
    "     'large_train': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/SMNI_CMI_TRAIN.tar.gz',\n",
    "     'large_test': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/SMNI_CMI_TEST.tar.gz',\n",
    "    #'full': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/eeg_full.tar'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifica se o diretório dos datasets existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando: SMNI_CMI_TEST.tar.gz ...\n",
      "Baixando: SMNI_CMI_TRAIN.tar.gz ...\n",
      "Downlod dos datasets concluído!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('dataset/'):\n",
    "    os.mkdir('dataset/')\n",
    "    for k, v in urls.items():\n",
    "        fn = v.split('/')[-1]\n",
    "        print('Baixando:', fn, '...')\n",
    "        urlretrieve(v, './dataset/{}'.format(fn))\n",
    "    print('Downlod dos datasets concluído!')\n",
    "else:\n",
    "    print('Dataset já baixado!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descompactando os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tar: dataset/eeg_full.tar: Cannot open: No such file or directory\\ntar: Error is not recoverable: exiting now'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.mkdir('dataset/eeg_full/')\n",
    "gop('tar -xvf dataset/eeg_full.tar -C dataset/eeg_full')\n",
    "#os.remove('dataset/eeg_full.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pastas de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = {\n",
    "    'small': 'dataset/small',\n",
    "    'large_train': 'dataset/large_train',\n",
    "    'large_test': 'dataset/large_test',\n",
    "    'full': 'dataset/full',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste de cada pessoa da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests =  [\"S1 obj\", \"S2 nomatch\", \"S2 match\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtra a data e remove os eletrodos desnecessários (x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalização dos dados e remoção x, y e nd\n",
    "def filterData(data):\n",
    "    newData = list()\n",
    "    scaler = StandardScaler()\n",
    "    for person in data:\n",
    "        newEletrodos = list()\n",
    "        for eletrodos in person:\n",
    "            eletrodos = np.delete(eletrodos, [31,62,63], axis=0)\n",
    "            scaler.fit(eletrodos)\n",
    "            newEletrodos.append(scaler.transform(eletrodos))\n",
    "        newData.append(np.array(newEletrodos))\n",
    "\n",
    "    return np.array(newData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aletera o shape da data de (x,y,z,k) para (xy,z,k) = (2,2,3,4) para (4,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mudança no shape\n",
    "def alterShape(data, tipo):\n",
    "    newData = list()\n",
    "    for person in data[0]:\n",
    "        for eletrodos in person:\n",
    "            newData.append(eletrodos)\n",
    "    data[0] = np.array(newData)\n",
    "    newData = None\n",
    "    print(\"data \"+tipo+\" shape: \",data[0].shape)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função principal que importa os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importNomalized(pathName, exp):\n",
    "    labelsList = list()\n",
    "    ch_names = []\n",
    "    create_ch_name = False\n",
    "    # carregando pasta \"large_train\"\n",
    "    path = gop('ls {}'.format(pathName)).split('\\n')\n",
    "    # 1ª dimensão dos dados contendo os sujeitos. Ex.: C_1, a_m, etc\n",
    "    subjects = list()\n",
    "    for types in path:\n",
    "        if(\"co2c\" in types):\n",
    "            for i in range (0,10):\n",
    "                labelsList.append([[0]]*61)\n",
    "        else:\n",
    "            for i in range (0,10):\n",
    "                labelsList.append([[1]]*61)\n",
    "\n",
    "        files = gop('ls {}/{}'.format(pathName, types)).split('\\n')\n",
    "        # 2ª dimensão dos dados contendo as sessões (trials)\n",
    "\n",
    "        trials = list()\n",
    "        for f in files:\n",
    "            arquivo = open('{}/{}/{}'.format(pathName, types, f))\n",
    "            text = arquivo.readlines()\n",
    "            # 3ª dimensão dos dados contendo os canais (eletrodos)\n",
    "            chs = list()\n",
    "            # 4ª dimensão dos dados contendo os valores em milivolts\n",
    "            values = list()\n",
    "            for line in text:\n",
    "                # ex: \"# FP1 chan 0\"\n",
    "                t = search('(?P<ch_name>\\w{1,3}) chan \\d{1,2}', line)\n",
    "                # ex: \"0 FP1 0 -8.921\"\n",
    "                p = search('^\\d{1,2}\\ \\w{1,3}\\ \\d{1,3}\\ (?P<value>.+$)', line)\n",
    "                if p:\n",
    "                    values.append(float(p.group('value')))\n",
    "                # mudou para outro eletrodo\n",
    "                elif t:\n",
    "                    if values:\n",
    "                        chs.append(values)\n",
    "                        values = list()\n",
    "                    if not create_ch_name:\n",
    "                        ch_names.append(t.group('ch_name').lower())\n",
    "            create_ch_name = True\n",
    "            chs.append(values)\n",
    "\n",
    "            arquivo.seek(32*3)\n",
    "            line =  arquivo.readline()\n",
    "            arquivo.close()\n",
    "\n",
    "            if exp in line:\n",
    "                trials.append(chs)\n",
    "\n",
    "        subjects.append(trials)\n",
    "    data = np.array(subjects)\n",
    "\n",
    "    return [filterData(data),np.asarray(labelsList)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(dataTrain):\n",
    "    #Cria um modelo sequencial com 3 camadas sendo a ultíma com saída binária\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 100, activation='relu', input_shape = (61,256)))\n",
    "    model.add(Dense(units = 50, activation='relu'))\n",
    "    model.add(Dense(units = 1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #Printa o modelo (camadas)\n",
    "    model.summary()\n",
    "\n",
    "    #74%\n",
    "    history = model.fit(dataTrain[0], dataTrain[1], nb_epoch=30)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testa os modelos construídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, dataTest):\n",
    "    return [(model.predict(dataTest[0]) > 0.5), (dataTest[1] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função que testa a precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prev, labels, test):\n",
    "    acerto = 0\n",
    "    erro = 0\n",
    "    eletrodo = 0\n",
    "\n",
    "    # labels = (labels == 1)\n",
    "\n",
    "    for pre, lab in zip(prev, labels):\n",
    "        for x1, x2 in zip(pre, lab):\n",
    "            if(x1[0] == x2[0]):\n",
    "                eletrodo+=1\n",
    "        if(eletrodo > 30):\n",
    "            acerto+=1\n",
    "        else:\n",
    "            erro+=1\n",
    "        eletrodo =0\n",
    "\n",
    "    print(\"Accuracy \"+test+\": \"+str((100*(acerto))/(acerto+erro)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP: S1 obj\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"dataset/large_train/ls: cannot access 'dataset/large_train': No such file or directory/ls: cannot access 'dataset/large_train/ls:': No such file or directory\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1de428773c2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EXP: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malterShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportNomalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'large_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"treino\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdataTst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malterShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportNomalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'large_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"teste\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-fff4551c5295>\u001b[0m in \u001b[0;36mimportNomalized\u001b[0;34m(pathName, exp)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0marquivo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marquivo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# 3ª dimensão dos dados contendo os canais (eletrodos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"dataset/large_train/ls: cannot access 'dataset/large_train': No such file or directory/ls: cannot access 'dataset/large_train/ls:': No such file or directory\""
     ]
    }
   ],
   "source": [
    "data = list()\n",
    "dataTst = list()\n",
    "models = list()\n",
    "results = list()\n",
    "\n",
    "for tst in tests:\n",
    "    print(\"EXP: \"+tst)\n",
    "    data.append(alterShape(importNomalized(folders['large_train'], tst), \"treino\"))\n",
    "    dataTst.append(alterShape(importNomalized(folders['large_test'], tst), \"teste\"))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    models.append(createModel(data.pop(0)))\n",
    "\n",
    "for model in models:\n",
    "    results.append(testModel(model, dataTst.pop(0)))\n",
    "\n",
    "for acc, test in zip(results,tests):\n",
    "    accuracy(acc[0],acc[1], test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
